{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src/')\n",
    "from functions import *\n",
    "from models import *\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from tensorboardX import SummaryWriter  \n",
    "#\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all parser arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Mode settings\n",
    "parser.add_argument('--train_mode', default=True, type=bool) # Set to False for Test mode.\n",
    "parser.add_argument('--load_model', default=False, type=bool)\n",
    "\n",
    "# Classes settings\n",
    "parser.add_argument('--dist_cut', default=10.0, type=float) # distance cut-off for class defintion\n",
    "parser.add_argument('--N_classes', default=2, type=int) # Number of classes\n",
    "parser.add_argument('--desired_labels', default=[0,1], type=list) # Classes to be considered for output\n",
    "\n",
    "# Bio-system settings\n",
    "parser.add_argument('--biosystem', default='PROTEIN', type=str) # Necessary for atom selections (see below)\n",
    "\n",
    "# Model settings\n",
    "parser.add_argument('--n_epochs', default=1000, type=int) # Number of epochs for training\n",
    "parser.add_argument('--batch_size', default=100, type=int) # Batch size for training\n",
    "parser.add_argument('--learning_rate', default=0.0002, type=float) # Learning rate for Adam optimizer\n",
    "parser.add_argument('--noise_dim', default=100, type=int) # Dimension for gaussian noise to feed to the generator\n",
    "\n",
    "# Output settings\n",
    "parser.add_argument('--desired_format', default='inpcrd', type=str)\n",
    "parser.add_argument('--epoch_freq', default=10, type=int) # Output will be generated every this many epochs\n",
    "parser.add_argument('--log_freq', default=100, type=int) # Verbose output will be printed every this many epochs\n",
    "parser.add_argument('--n_structures', default=10, type=int) # Number of structures to be generated for every class (only in Test mode)\n",
    "parser.add_argument('--input_directory', default='./example_input/' , type=str) \n",
    "parser.add_argument('--output_directory', default='./example_output/', type=str) \n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "prmf = args.input_directory+'peptide.prmtop' # Parameter and topology file\n",
    "#trajfs = [args.input_directory+'all_conformations.mdcrd'] # Trajectory files list\n",
    "trajfs = ['/home/acrnjar/Desktop/TEMP/Peptides_gen/all_conformations.mdcrd'] # Trajectory files list\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Cuda is available:\",torch.cuda.is_available())\n",
    "\n",
    "### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"Output will be written in:\",args.output_directory)\n",
    "    if not os.path.exists(args.output_directory): os.system('mkdir '+args.output_directory)\n",
    "\n",
    "    if args.biosystem=='PROTEIN':\n",
    "        backbone='name CA C N'\n",
    "    elif args.biosystem=='DNA':\n",
    "        backbone='name P'\n",
    "\n",
    "    print(\"train_mode:\",args.train_mode)\n",
    "    print(\"load_model:\",args.load_model)\n",
    "\n",
    "    model_g_file=args.output_directory+'model_generator.pth'\n",
    "    model_d_file=args.output_directory+'model_discriminator.pth'\n",
    "\n",
    "    # Remove previous output if necessary\n",
    "    if args.train_mode:\n",
    "        os.system('rm '+args.output_directory+'gen*'+args.desired_format+' 2> /dev/null')\n",
    "        if not args.load_model:\n",
    "            os.system('rm '+args.output_directory+'out*'+args.desired_format+' 2> /dev/null')\n",
    "\n",
    "    # Determine last saved epoch   \n",
    "    last_epoch=0\n",
    "    if args.train_mode and args.load_model:\n",
    "        prefixed = [filename for filename in os.listdir(args.output_directory) if filename.startswith(\"out_train_label1_epoch\")]\n",
    "        past_epochs=[]\n",
    "        for wfile in prefixed:\n",
    "            past_epochs.append(int(wfile.replace('out_train_label1_epoch','').replace('.'+desired_format,'')))\n",
    "        last_epoch=max(past_epochs)\n",
    "        print(\"Last epoch found:\",last_epoch)\n",
    "\n",
    "    # Define MDAnalysis universe and related parameters\n",
    "    univ = mda.Universe(prmf, trajfs)\n",
    "    nframes = len(univ.trajectory)\n",
    "    batch_freq = int(nframes/args.batch_size) \n",
    "    N_at = len(univ.select_atoms('all'))\n",
    "    box_s = max_size(prmf,trajfs,'all',1.1) # Calculate largest coordinate for generation\n",
    "\n",
    "    # Generate data\n",
    "    dataset,atoms_list = generate_training_data(prmf, trajfs, 0, nframes-(nframes%args.batch_size), backbone, args.dist_cut, args.output_directory)\n",
    "\n",
    "    # Define discriminator and generator models\n",
    "    discriminator = DiscriminatorModel(N_at, args.N_classes, n1=50, n2=100, n3=200) \n",
    "    generator = GeneratorModel(N_at, args.noise_dim, args.N_classes, box_s, n1=200, n2=100, n3=50) \n",
    "    discriminator.to(device)\n",
    "    generator.to(device)\n",
    "    if (args.load_model==True and os.path.isfile(model_g_file) and os.path.isfile(model_d_file)):\n",
    "        print(\"{} and {} loaded.\".format(model_g_file,model_d_file))\n",
    "        generator.load_state_dict(torch.load(model_g_file))\n",
    "        discriminator.load_state_dict(torch.load(model_d_file))\n",
    "\n",
    "    # For a conditional GAN, the loss function is the Binary Cross Entropy between the target and the input probabilities\n",
    "    loss = nn.BCELoss() \n",
    "\n",
    "    # Define optimizers\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=args.learning_rate) \n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # Load data\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    if args.train_mode:\n",
    "        summary_writer = SummaryWriter(args.output_directory)\n",
    "        \n",
    "        # initialize lists for observables and graphs.\n",
    "        e2e_distance = []\n",
    "        bonds_dev = []\n",
    "        angles_dev = []\n",
    "        losses_fig = plt.figure(1, figsize=(4, 4))\n",
    "        e2e_fig = plt.figure(1, figsize=(4, 4))\n",
    "        bonds_fig = []\n",
    "        angles_fig = []\n",
    "        for dl, d_label in enumerate(args.desired_labels):\n",
    "            e2e_distance.append([])\n",
    "            bonds_dev.append([])\n",
    "            angles_dev.append([])\n",
    "            bonds_fig.append(plt.figure(1, figsize=(4, 4)))\n",
    "            angles_fig.append(plt.figure(1, figsize=(4, 4)))\n",
    "        \n",
    "        Loss_G_mean = []\n",
    "        Loss_D_mean = []\n",
    "        for epoch_idx in range(args.n_epochs): \n",
    "            G_loss = [] \n",
    "            D_loss = []    \n",
    "            for batch_idx, data_input in enumerate(data_loader):\n",
    "            \n",
    "                # Generate noise and move it the device\n",
    "                noise = torch.randn(args.batch_size,args.noise_dim).to(device) \n",
    "                # Forward pass\n",
    "                fake_labels = torch.randint(0,args.N_classes,(args.batch_size,)).to(device)\n",
    "                generated_data = generator(noise, fake_labels) \n",
    "                \n",
    "                true_data = data_input[0].view(args.batch_size, 3*N_at).to(device) \n",
    "                digit_labels = data_input[1].to(device) \n",
    "                true_labels = torch.ones(args.batch_size).to(device) \n",
    "\n",
    "                # Clear optimizer gradients        \n",
    "                discriminator_optimizer.zero_grad()\n",
    "                # Forward pass with true data as input\n",
    "                discriminator_output_for_true_data = discriminator(true_data,digit_labels).view(args.batch_size) \n",
    "                # Compute Loss\n",
    "                true_discriminator_loss = loss(discriminator_output_for_true_data, true_labels) \n",
    "                \n",
    "                # Forward pass with generated data as input\n",
    "                discriminator_output_for_generated_data = discriminator(generated_data.detach(), fake_labels).view(args.batch_size) \n",
    "                # Compute Loss\n",
    "                generator_discriminator_loss = loss(discriminator_output_for_generated_data, torch.zeros(args.batch_size).to(device)) \n",
    "                # Average the loss\n",
    "                discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2 \n",
    "                # Backpropagate the losses for Discriminator model.\n",
    "                discriminator_loss.backward()\n",
    "                discriminator_optimizer.step()\n",
    "                D_loss.append(discriminator_loss.data.item())\n",
    "                \n",
    "                # Clear optimizer gradients\n",
    "                generator_optimizer.zero_grad()        \n",
    "                # It's a choice to generate the data again \n",
    "                generated_data = generator(noise, fake_labels) #.requires_grad_(False) \n",
    "                # Forward pass with the generated data\n",
    "                discriminator_output_on_generated_data = discriminator(generated_data, fake_labels).view(args.batch_size) \n",
    "                # Compute loss: it must be the same of the discriminator, but reversed: the fake data must be passed as all ones, thus we use true_labels\n",
    "                generator_loss = loss(discriminator_output_on_generated_data, true_labels) \n",
    "                # Backpropagate losses for Generator model.\n",
    "                generator_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "                G_loss.append(generator_loss.data.item())\n",
    "                \n",
    "                if (batch_idx==0 and epoch_idx==0): print(\"Initial: discriminator_loss: {} , generator_loss: {}\".format(discriminator_loss.item(),generator_loss.item()))\n",
    "\n",
    "                # Evaluate the model\n",
    "                if ((batch_idx + 1)% batch_freq == 0 and (epoch_idx + 1)%args.epoch_freq == 0): \n",
    "                    with torch.no_grad(): \n",
    "                        noise = torch.randn(args.batch_size,args.noise_dim).to(device)\n",
    "                        for dl, d_label in enumerate(args.desired_labels):\n",
    "                            fake_labels = torch.tensor(args.batch_size*[dl]).to(device) \n",
    "                            generated_data = generator(noise, fake_labels).cpu().view(args.batch_size, 3*N_at) \n",
    "                            for x in generated_data:\n",
    "\n",
    "                                # Generate .inpcrd file\n",
    "                                outname='out_label'+str(fake_labels[0].item())+'_epoch'+str(last_epoch+epoch_idx+1)+'.inpcrd'\n",
    "                                write_inpcrd(x.detach().numpy().reshape(N_at,3),outname=args.output_directory+outname)\n",
    "                                if (epoch_idx+1)%args.log_freq==0: print(\"{} written.\".format(outname))\n",
    "\n",
    "                                # Calculate observables for later evaluation of the training\n",
    "                                e2e_distance[dl].append([epoch_idx,check_label_condition(prmf,args.output_directory+outname)])\n",
    "                                bonds_dev[dl].append([epoch_idx,bonds_deviation(prmf,args.output_directory+outname)])\n",
    "                                angles_dev[dl].append([epoch_idx,angles_deviation(prmf,args.output_directory+outname)]) \n",
    "                                torch.save(generator.state_dict(),model_g_file) \n",
    "                                torch.save(discriminator.state_dict(),model_d_file) \n",
    "                                break\n",
    "                    \n",
    "            if (epoch_idx+1)%args.epoch_freq==0:\n",
    "                summary_writer.add_scalar('Loss_d',torch.mean(torch.FloatTensor(D_loss)),global_step=epoch_idx)\n",
    "                summary_writer.add_scalar('Loss_g',torch.mean(torch.FloatTensor(G_loss)),global_step=epoch_idx)\n",
    "                Loss_D_mean.append([epoch_idx,torch.mean(torch.FloatTensor(D_loss))])\n",
    "                Loss_G_mean.append([epoch_idx,torch.mean(torch.FloatTensor(G_loss))])\n",
    "\n",
    "            if (epoch_idx+1)%args.log_freq==0:\n",
    "                print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % ( (epoch_idx+last_epoch+1), args.n_epochs+last_epoch, torch.mean(torch.FloatTensor(D_loss)), torch.mean(torch.FloatTensor(G_loss))))\n",
    "\n",
    "        # Plot loss averages over batches\n",
    "        plt.plot(np.array(Loss_D_mean)[:, 0], np.array(Loss_D_mean)[:, 1],lw=1,c='C0',label='Discriminator')\n",
    "        plt.plot(np.array(Loss_G_mean)[:, 0], np.array(Loss_G_mean)[:, 1],lw=1,c='C1',label='Generator')\n",
    "        plt.legend(loc='upper right',prop={'size':15})\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        losses_fig.savefig(args.output_directory+'Losses.png',dpi=150)\n",
    "        #plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot observables\n",
    "        for dl, d_label in enumerate(args.desired_labels):\n",
    "            plt.plot(np.array(bonds_dev[dl])[:, 0], np.array(bonds_dev[dl])[:, 1],lw=1,c='C0')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Bonds dev. [$\\AA$]')\n",
    "            bonds_fig[dl].savefig(args.output_directory+'Bonds_deviation_label'+str(d_label)+'.png',dpi=150)\n",
    "            #plt.show()\n",
    "            plt.clf()\n",
    "            plt.plot(np.array(angles_dev[dl])[:, 0], np.array(angles_dev[dl])[:, 1],lw=1,c='C1')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Angle dev. [deg]')\n",
    "            angles_fig[dl].savefig(args.output_directory+'Angles_deviation_label'+str(d_label)+'.png',dpi=150)\n",
    "            #plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        # Plot the end-to-end distances\n",
    "        for dl, d_label in enumerate(args.desired_labels):\n",
    "            plt.plot(np.array(e2e_distance[dl])[:, 0], np.array(e2e_distance[dl])[:, 1],lw=1,c='C'+str(dl),label='Label '+str(d_label))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('End-to-end distance [$\\AA$]')\n",
    "        plt.legend(loc='upper right',prop={'size':15})\n",
    "        e2e_fig.savefig(args.output_directory+'End2end_distances.png',dpi=150)\n",
    "        #plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "    # Test mode\n",
    "    else: \n",
    "        for structure_idx in range(args.n_structures):\n",
    "            print(\"Generating structure:\",structure_idx)\n",
    "            with torch.no_grad(): \n",
    "                noise = torch.randn(args.batch_size,args.noise_dim).to(device)\n",
    "                for dl,d_label in enumerate(args.desired_labels):\n",
    "                    fake_labels = torch.tensor(args.batch_size*[dl]).to(device) \n",
    "                    generated_data = generator(noise, fake_labels).cpu().view(args.batch_size, 3*N_at) \n",
    "                    for x in generated_data:\n",
    "                        outname='gen_label'+str(fake_labels[0].item())+'_'+str(structure_idx+1)+'.inpcrd' \n",
    "                        write_inpcrd(x.detach().numpy().reshape(N_at,3),outname=args.output_directory+outname)\n",
    "                        print(\"{} written.\".format(outname))\n",
    "                        break\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5adb80dd3fccf15d80bd42e63c435d5dac4d8dc19a7ae9759e72dc58a9d17bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
